{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DH1XQ07F5Bx8"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will get started by importing and cleaning the LendingClub dataset, which will be used throughout the course.\n",
    "\n",
    "We will achieve this by leveraging key data cleaning techniques in Python using Pandas, among other libraries.\n",
    "\n",
    "\n",
    "## Agenda:\n",
    "1. Importing data for your Data Science project\n",
    "2. Understanding your data and cleaning it:initial exploration and stats\n",
    "3. Handling missing values\n",
    "4. Removing duplicates\n",
    "5. Handling outliers\n",
    "6. Text data handling\n",
    "7. Intro to time series data\n",
    "\n",
    "\n",
    "Demo: Implementation in Python\n",
    "------------------------------\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtogBfReQgxs"
   },
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmGuFdipQgxv"
   },
   "source": [
    "#### User-specified parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1725601239299,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "iF1cKwF4Qgxw"
   },
   "outputs": [],
   "source": [
    "python_material_folder_name = \"python-material\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iej9FzGAQgxy"
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29528,
     "status": "ok",
     "timestamp": 1725601270227,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "MR78M58RQgx0",
    "outputId": "ca2dd596-8976-42b5-dd96-67e0816ea09f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Check if in Google Colab environment\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    # Mount drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Set up path to Python material parent folder\n",
    "    path_python_material = rf\"drive/MyDrive/{python_material_folder_name}\"\n",
    "        # If unsure, print current directory path by executing the following in a new cell:\n",
    "        # !pwd\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    # If working locally on Jupyter Notebook, parent folder is one folder up (assuming you are using the folder structure shared at the beginning of the course)\n",
    "    path_python_material = \"..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPHTDE_ZQgx2"
   },
   "source": [
    "1\\. Importing data for your Data Science project\n",
    "----------------------------------------\n",
    "\n",
    "### Flat file (.csv) format\n",
    "Our data is stored in csv format which is a very popular tabular format, where each line corresponds to a row, and each field within the row is separated by a comma.\n",
    "\n",
    "Csv is widely used for data exchange and storage, especially for datasets that need to be easily readable and editable with spreadsheet software (like Microsoft Excel) and programming languages (like Python, R).\n",
    "\n",
    "- Advantages: Simple, easy to create and parse, human-readable.\n",
    "- Disadvantages: Can become large and unwieldy with big datasets, lacks support for complex data structures.\n",
    "\n",
    "Let's read our data in using [**pandas**](https://pandas.pydata.org/docs/getting_started/index.html#getting-started), Python's most popular library for data manipulation.\n",
    "\n",
    "**Note:** Thorughout this course, data will be located in the *data* folder within the code area.\n",
    "\n",
    "### Other formats\n",
    "- See [data import cheat sheet(text files, SAS files, Excel files, relational databases, etc.](https://www.datacamp.com/cheat-sheet/importing-data-in-python-cheat-sheet)\n",
    "- [Read data from APIs](https://medium.com/analytics-lane/python-get-and-process-web-api-data-through-pandas-and-requests-part-1-32127638b463)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "executionInfo": {
     "elapsed": 5819,
     "status": "ok",
     "timestamp": 1725601423528,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "Ev6Xsmi_Qgx4",
    "outputId": "1e57df8c-f133-4848-8167-faf31d385b3f"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Note: optionally, added a 'sample' step to reduce size if facing slowdown/computational challenges.\n",
    "df = pd.read_csv(rf\"{path_python_material}/data/1-raw/lending-club-2007-2020Q3/Loan_status_2007-2020Q3-100ksample.csv\")#.sample(100)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1725601435472,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "eR6xGIvjQgx6",
    "outputId": "8c04b32d-35fa-40db-8fad-e0c81741ea14"
   },
   "outputs": [],
   "source": [
    "df.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1725601441887,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "2X6oLbeUQgyA",
    "outputId": "5fb2c665-6ac6-4bc8-9ce6-01211771a62c"
   },
   "outputs": [],
   "source": [
    "df.dtypes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbZCSclGQgyA"
   },
   "source": [
    "**Hint**: Check out data types [here](https://pbpython.com/pandas_dtypes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BYT-ok_QgyB"
   },
   "source": [
    "### <span style=\"color:BLUE\"> **>>> DISCUSSION: What are some of the things you notice by just looking at the data?**  </span>    \n",
    "*Hints*:\n",
    "- You may want to take a look at the data dictionary provided in the 'data' folder\n",
    "- Do you understand what each column, and the values within it, is or means?\n",
    "- Do you notice any inconsistencies or potential areas of concern around data quality or completeness?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZpDLrAnQgyB"
   },
   "source": [
    "2\\. Understanding your data and cleaning it: initial exploration and stats\n",
    "----------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFe629CuQgyB"
   },
   "source": [
    "As you can probably tell from looking at the above, it is easy to get overwhelmed by trying to scroll through over 100 fields to make sense of the data!\n",
    "In a real-life scenario, we would want to make sure we spend enough time (weeks if needed!) to deeply understand it and make sure it is ready to be used, by applying some of the techniques covered over the next few sessions.\n",
    "\n",
    "Let's get started by removing some of the columns that are not needed - this will make our life easier.\n",
    "For that, we will be using the pandas 'drop' method, which takes a list of columns to be dropped as follows:\n",
    "``` df.drop(labels)``` : where labels is a single column name, or list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JinW-yHSQgyC"
   },
   "source": [
    "### <span style=\"color:BLUE\"> **>>> EXERCISE:**  </span>\n",
    "> - Pick 2 columns you would drop from this dataset (be ready to explain why). Create a list called `to_drop`.\n",
    "> - Drop the columns from your df by passing the list. If you are unsure how to do this, you are allowed to search on Google for the relevant pandas method - it is good to get familiar with searching through documentation.\n",
    "> - The new dataframe should be called `df_dropped`.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1725602025189,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "egPlgOA1QgyC"
   },
   "outputs": [],
   "source": [
    "# list_to_drop = #YOUR CODE HERE\n",
    "# df_dropped = #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1725602025959,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "e1bFFw9kQgyD",
    "outputId": "d03cb136-c82d-4ae2-d746-cef3feac1dd0"
   },
   "outputs": [],
   "source": [
    "print(f\"df shape: {df.shape}\")\n",
    "print(f\"df_dropped shape: {df_dropped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJKtp_BAQgyD"
   },
   "source": [
    "**When to consider dropping columns?**\n",
    "- **business/problem relevance**: is the data relevant to the problem I am trying to solve (e.g. Do I need IDs?)\n",
    "- **high % of missing values**: especially if the data is not easily recoverable or imputation might introduce significant bias.\n",
    "- **little to no variance**: provide minimal information for the model and can be dropped.\n",
    "- **high correlation**: if two or more columns are highly correlated, they contain redundant information. You might drop one of them to reduce 'multicollinearity'.\n",
    "- **high cardinality**: Categorical columns with too many unique values (high cardinality) can lead to overfitting and increased computational cost. These can be dropped or encoded differently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIfAqnZYQgyE"
   },
   "source": [
    "\n",
    "### Basic data exploration and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1725602079493,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "QpFOiLh2QgyF",
    "outputId": "7bb02f9a-31f6-43ba-fb2d-58c3b1dbb2e9"
   },
   "outputs": [],
   "source": [
    "# Number of rows and columns\n",
    "df_dropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9a84oNsQgyG"
   },
   "source": [
    "#### Numeric columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbZ8T6jlQgyG"
   },
   "source": [
    "The dataset used has 100k rows - for the purposes of this course, a sample is being used for simplicity (original dataset size: circa 3m records covering period between 2007 and 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsXNYXJLQgyG"
   },
   "source": [
    "The describe() method generates descriptive statistics **for numeric columns**. This includes:\n",
    "\n",
    "- count: Number of non-null entries.\n",
    "- mean: Average value.\n",
    "- std: Standard deviation.\n",
    "- min: Minimum value.\n",
    "- 25%: 25th percentile (first quartile).\n",
    "- 50%: 50th percentile (median or second quartile).\n",
    "- 75%: 75th percentile (third quartile).\n",
    "- max: Maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "executionInfo": {
     "elapsed": 1606,
     "status": "ok",
     "timestamp": 1725602094297,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "NqdT14k_QgyH",
    "outputId": "a1d301a0-95a8-477f-d92a-09ff052efd9f"
   },
   "outputs": [],
   "source": [
    "# Descriptive statistics of the numeric columns\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "df_dropped.describe().apply(lambda x: x.apply('{0:.2f}'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69nLtG7sQgyH"
   },
   "source": [
    "This table can be slightly overwhelming to digest, but is very useful to get a sense of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUWcHgUsQgyI"
   },
   "source": [
    "#### Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1725602198333,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "3AYZUCIvQgyI",
    "outputId": "633d3aae-fa0a-4ceb-95cb-e5da8e17f560",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_colunmns = df_dropped.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"\\nNumber of unique values in each column:\")\n",
    "df_dropped[categorical_colunmns].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZHxmRQ7QgyI"
   },
   "source": [
    "This can be used to further validate our understanding of the data, as well as detect unusual or rare values that may indicate data entry errors, outliers, or irrelevant categories.\n",
    "\n",
    "E.g.:\n",
    "- Are we expecting single value of identifiers per record (linked to granularity)\n",
    "- Does any of the categorical features present too many values, and if so could it be be simplified by grouping them into fewer categories ('enconding')?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaVl3fB0QgyJ"
   },
   "source": [
    "Let's look at the values assumed by one of the categorical features.  \n",
    "\n",
    "One of the variables to look out for is *loan_status*, which will be used for modelling later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1725602267987,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "f4cXbI6iQgyJ",
    "outputId": "526c68b2-c8c0-4602-d12c-414f247c504a"
   },
   "outputs": [],
   "source": [
    "df_dropped.loan_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5eWN7fpQgyJ"
   },
   "source": [
    "*Useful definitions:*  \n",
    "- *Defaulted Loan: A loan is considered in default when the borrower fails to make the required payments as agreed in the loan contract. Default typically occurs after missing several payments (usually 90 to 180 days, depending on the type of loan and lender policies).*\n",
    "- *A loan is charged off when the lender writes off the loan as a bad debt on their financial statements, recognizing it as a loss. This typically happens after the loan has been in default for a significant period, often around 180 days.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h75K04w3QgyK"
   },
   "source": [
    "### <span style=\"color:BLUE\"> **>>> EXERCISE:**  </span>    \n",
    "> Calculate the average loan amount for loans that were charged off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufStEjuqQgyK"
   },
   "outputs": [],
   "source": [
    "# # Step 1 - Filter on charged off records based on df_dropped.loan_status == \"Charged Off\" condition\n",
    "# charged_off_loans = # YOUR CODE HERE\n",
    "#\n",
    "# # Step 2 - Calculate average size of charged off loan\n",
    "# average_size_loan = # YOUR CODE HERE\n",
    "# print(f\"\"\"Average size of charged off loan:\n",
    "#           {\n",
    "#               average_size_loan\n",
    "#           }\"\"\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xmQ8a3MQgyN"
   },
   "outputs": [],
   "source": [
    "# Tip: How to format your output?\n",
    "formatted_average_size_loan = f\"${average_size_loan:,.2f}\"\n",
    "formatted_average_size_loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1725602405004,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "KIvaolXoQgyN"
   },
   "outputs": [],
   "source": [
    "# Let's apply to the entire dataframe\n",
    "\n",
    "def format_dollar(amount):\n",
    "    return f\"${amount:,.2f}\"\n",
    "\n",
    "# Apply the function to the 'loan_amount' column for display purposes\n",
    "df_dropped['loan_amount_formatted'] = df_dropped['loan_amnt'].apply(format_dollar)\n",
    "df_dropped['loan_amount_formatted'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbC-KGGVQgyO"
   },
   "source": [
    "Note: The above number has been formatted to string, hence while this may be useful to display the result - it will not be useful for analysis/modelling purposes, as seen in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "executionInfo": {
     "elapsed": 3544,
     "status": "ok",
     "timestamp": 1725602415754,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "gcusaoG4QgyP",
    "outputId": "20d40c52-78c6-4650-f3a9-3ef58c2b1557"
   },
   "outputs": [],
   "source": [
    "# As an example:\n",
    "df_dropped['loan_amount_formatted'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyWAEofkQgyW"
   },
   "source": [
    "### <span style=\"color:BLUE\"> **>>> DISCUSSION: Can you guess what happened and why?**  </span>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n96TIjIQgyX"
   },
   "source": [
    "### <span style=\"color:BLUE\"> **>>> EXERCISE:**  </span>    \n",
    "> Drop the `loan_amount_formatted` column given it will not be useful for our downstream work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc8qjp9yQgyX"
   },
   "outputs": [],
   "source": [
    "# df_dropped = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wT8YVz08QgyY"
   },
   "source": [
    "3\\. Handling missing values\n",
    "----------------------------------------\n",
    "\n",
    "\n",
    "Identifying and treating missing value is a critical step in data preparation, particularly given many algorithms require a complete dataset to calculate distances, probabilities, and optimize objective functions. It is also a great opportunity to cement our understanding of the features and what they represent.\n",
    "\n",
    "In Pandas, missing data is represented by two values:\n",
    "- None: a Python singleton object that is often used for missing data in Python code.\n",
    "- NaN : acronym for 'Not a Number' - is a special floating-point value recognized by all systems that use the standard IEEE floating-point representatio\n",
    "\n",
    "**Explanation**\n",
    "Missing values are common in real-world datasets and need to be handled appropriately. There are several ways to deal with missing values:\n",
    "- Removal: Dropping rows or columns with missing values.\n",
    "- Imputation: Filling missing values with a specific value, such as the mean, median, or mode.\n",
    "\n",
    "**Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1725602428483,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "9ShliHr0QgyY",
    "outputId": "ad831fee-d0b9-453e-e9a1-344b1d951694"
   },
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "df_dropped.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2qkj1kPQgyZ"
   },
   "source": [
    "#### Example approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI3ExVpOQgyZ"
   },
   "source": [
    "We could leverage the power of Python to create a rule that automatically drops columns with a % of missing values that exceeds a given threshold. However, we **absolutely** need to make sure we understand the data and what missing values **actually** represent.\n",
    "\n",
    "If unsure, do not drop features at this initial stage, as we will be learning about techniques that can help with feature selection when we get to model building classes.\n",
    "\n",
    "As an example, let's image data SMEs in the business have advised that 'hardship' related features actually point to customers who are NOT on hardship plan, which may actually be useful information we do not want to discard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnvM07PQQgyZ"
   },
   "outputs": [],
   "source": [
    "# Create hardship flag based on information provided by data SMEs in the business\n",
    "df_dropped['hardship_status_filled'] = df_dropped['hardship_status'].fillna(\"NO_HARDSHIP\")\n",
    "print(f\"Nulls after filling: {df_dropped['hardship_status_filled'].isnull().sum()}\")\n",
    "df_dropped[['hardship_status_filled', 'hardship_status']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HRf1dLWQgyZ"
   },
   "outputs": [],
   "source": [
    "# Imputation: Filling missing values with the mean for numerical columns\n",
    "df_dropped['num_accts_ever_120_pd'].fillna(df_dropped['num_accts_ever_120_pd'].mean(), inplace=True)\n",
    "print(f\"Nulls after filling: {df_dropped['num_accts_ever_120_pd'].isnull().sum()}\")\n",
    "df_dropped[['num_accts_ever_120_pd']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcTKcbzEQgya"
   },
   "source": [
    "Alternatively, a more radical approach is to drop records when null values appear. This is **not recommended** unless you have confidence that it will meet a business need, or not impact your modelling too much (e.g. overall data is populated, only minor portion of rows have missing values which we want to discard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQu0bE4oQgya"
   },
   "outputs": [],
   "source": [
    "# Let's try this 'blanket' approach on our data and see what happens:\n",
    "df_blanket_drop = df_dropped.dropna(axis=0, how='any')\n",
    "df_blanket_drop.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZQYvdtFQgya"
   },
   "source": [
    "As expected, we ended up with very few/no records given the many 'holes' in our dataset. This is very common in practice for such large data sources (and so many features which are often sparsely populated), so if really wanted to use this approach, we may want to restrict dropping based on a subset of features being null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJmSRcvBQgyb"
   },
   "outputs": [],
   "source": [
    "# Let's try this 'blanket' approach on a subset of fields and see what happens:\n",
    "df_blanket_drop2 = df_dropped.dropna(axis=0, how='any',subset=['mo_sin_rcnt_rev_tl_op', 'emp_title'])\n",
    "df_blanket_drop2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTjI3tFMQgyd"
   },
   "source": [
    "Alternatively, we can use the 'all' option to only drop records that are completely empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5s0IvMGQgyd"
   },
   "outputs": [],
   "source": [
    "# Let's try this option and see what happens:\n",
    "df_blanket_drop3 = df_dropped.dropna(axis=0, how='all')\n",
    "df_blanket_drop3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u95kBxJSQgye"
   },
   "source": [
    "### <span style=\"color:BLUE\"> **>>> DISCUSSION:**  </span>    \n",
    "- Can you think of benefits / limitations of above approaches?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YD5x4-6Qgyi"
   },
   "source": [
    "4\\. Removing duplicates\n",
    "----------------------------------------\n",
    "\n",
    "Duplicates can skew your analysis and should be removed to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZ1yORvOQgyn"
   },
   "outputs": [],
   "source": [
    "# Checking for duplicate rows\n",
    "duplicates = df_dropped.duplicated().sum()\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR3zJLvEQgyo"
   },
   "outputs": [],
   "source": [
    "# Removing duplicate rows\n",
    "df_deduped = df_dropped.drop_duplicates(inplace=True)\n",
    "\n",
    "try:\n",
    "    df_deduped.shape # Note: it will fail if empty (i.e. no duplicates)\n",
    "except:\n",
    "    print(\"No duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ0Of17CQgyp"
   },
   "source": [
    "### <span style=\"color:BLUE\"> **>>> DISCUSSION:**  </span>    \n",
    "- Can you think of benefits / limitations of above approaches?\n",
    "- Do you know why we used the 'try' and 'except' logic?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NULBAYdOQgyq"
   },
   "source": [
    "5\\. Handling outliers\n",
    "----------------------------------------\n",
    "Outliers can distort statistical analysis and models. They can be genuine outlying observations (e.g.: Black Friday sales spike), or point to data errors (e.g.: manual data entry). In both cases, we would want to account for this and treat them.\n",
    "\n",
    "Common techniques to **identify** outliers include:\n",
    "\n",
    "- Z-score: Identifying outliers based on the standard deviation.\n",
    "- IQR (Interquartile Range): Identifying outliers based on the 25th and 75th percentiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOTfM9o7Qgyq"
   },
   "source": [
    "### Why is Outlier Detection Important?\n",
    "\n",
    "Outliers can significantly impact the performance and accuracy of machine learning models and statistical analyses. Here's why detecting and handling outliers is crucial:\n",
    "\n",
    "1\\. **Model Performance:** Outliers can skew model training and lead to poor generalization on unseen data.\n",
    "\n",
    "2\\. **Data Integrity:** Outliers may indicate data entry errors or unusual events that require further investigation.\n",
    "\n",
    "3\\. **Statistical Accuracy:** Outliers can affect summary statistics such as mean, standard deviation, and correlation, leading to misleading conclusions.\n",
    "\n",
    "4\\. **Improved Insights:** Identifying outliers can reveal valuable insights into rare but important events or patterns in the data.\n",
    "\n",
    "### Use Cases for Outlier Detection\n",
    "\n",
    "In the context of lending, outlier detection can be used in several ways:\n",
    "\n",
    "1\\. **Fraud Detection:** Outliers in financial transactions or application data could indicate fraudulent activities.\n",
    "\n",
    "2\\. **Credit Risk Analysis:** Unusual loan amounts, incomes or other features (or set of) might signal higher risk applications.\n",
    "\n",
    "3\\. **Data Quality Improvement:** Identifying and correcting outliers can improve the overall quality of the dataset.\n",
    "\n",
    "### Example Analysis on LendingClub Loan Application Data\n",
    "\n",
    "Let's perform an outlier detection analysis on LendingClub loan application data using various techniques.\n",
    "\n",
    "#### Checking for Outliers using Z-score\n",
    "\n",
    "The Z-score method identifies outliers by measuring how many standard deviations a data point is from the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0SNZ738Qgyr"
   },
   "source": [
    "From [investopedia](https://www.investopedia.com/terms/z/zscore.asp#:~:text=Understanding%20Z%2DScore,-Z%2Dscore%20is&text=It%20indicates%20how%20many%20standard,standard%20deviation%20from%20the%20mean.):\n",
    "> Z-score is a statistical measure that quantifies the distance between a data point and the mean of a dataset. It's expressed in terms of standard deviations. It indicates how many standard deviations a data point is from the mean of the distribution.\n",
    "\n",
    "> If a Z-score is 0, it indicates that the data point's score is identical to the mean score. A Z-score of 1.0 would indicate a value that is one standard deviation from the mean. Z-scores may be positive or negative, with a positive value indicating the score is above the mean and a negative score indicating it is below the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHhAYJGrQgyr"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculating on a single column, e.g. loan amount\n",
    "z_scores = stats.zscore(df_dropped[\"loan_amnt\"])\n",
    "sns.displot(z_scores, bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRxIufJpQgyu"
   },
   "source": [
    "Observations:\n",
    "- Loan amount follows a distribution close to normal (a bit 'skewed' to the left)\n",
    "- Do we have any outliers? This depends on how we define outlier (ie. where we set the threshold), as per example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGCVnmr4Qgyv"
   },
   "outputs": [],
   "source": [
    "# Threshold = 3\n",
    "for threshold in range(200, 301, 25):\n",
    "    thresh = threshold/100\n",
    "    print(f\"Threshold == {thresh}, {df_dropped[np.abs(z_scores)>thresh].shape[0]} outliers \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sk9K7u2lQgyv"
   },
   "source": [
    "Now let's calculate across all numeric features, no longer single feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFjBvuS7Qgyv"
   },
   "outputs": [],
   "source": [
    "# Calculate Z-scores for numerical columns\n",
    "z_scores = np.abs(stats.zscore(df_dropped.select_dtypes(include=[np.number])))\n",
    "\n",
    "# Define a threshold for identifying outliers\n",
    "threshold = 3\n",
    "\n",
    "# How many outliers identified for each numerical columns?\n",
    "gt_thresh = z_scores > threshold\n",
    "outlier_volume = pd.DataFrame(gt_thresh.sum(), columns=[\"num_outliers\"])\n",
    "outlier_volume.sort_values(by = \"num_outliers\"\n",
    "                          , ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRVMrXGvQgyw"
   },
   "source": [
    "### <span style=\"color:BLUE\"> **>>> EXERCISE:**  </span>\n",
    "> Highlight the outlying values for `last_pmnt_amnt`. How do they compare to the rest of the distribution for that same column?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfMGX1IBQgyw"
   },
   "outputs": [],
   "source": [
    "# # YOUR CODE HERE\n",
    "# lpa_outliers = # YOUR CODE HERE\n",
    "# print(f\"Outlying values:\\n {len(lpa_outliers)}\")\n",
    "\n",
    "# print(f\"Distribution details for outliers:\\n {#YOUR CODE HERE}\")\n",
    "# print(f\"Distribution details for non-outliers:\\n {#YOUR CODE HERE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEy1BhiEQgyy"
   },
   "source": [
    "### Visualizing Outliers\n",
    "Visualizing outliers can help understand their distribution and impact.\n",
    "Boxplots (a.k.a. box and whisker plots) can be used to visualise distributions and outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sy-1oBPqQgyy"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "field_to_plot = \"last_pymnt_amnt\"\n",
    "\n",
    "# Plotting variable distribution with outliers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=df_dropped[field_to_plot])\n",
    "plt.title(f\"Distribution and outliers for {field_to_plot}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaYU0eNvQgy1"
   },
   "source": [
    "Do you notice anything unusual in the chart above?\n",
    "\n",
    "The number of outliers is indeed different - this is because the library used adopts a differnt methodology to identify outliers, based on Interquantile Range. You can read more details [here](https://www.geeksforgeeks.org/interquartile-range-to-detect-outliers-in-data/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AICEtmFjQgy2"
   },
   "source": [
    "\n",
    "\n",
    "### Once detected, handling outliers\n",
    "\n",
    "Once outliers have been identified - what do we do with them?\n",
    "\n",
    "Outliers can be handled in several ways, including removal, transformation, or capping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZZnEtxvQgy2"
   },
   "outputs": [],
   "source": [
    "# Option 1 - Removing outliers\n",
    "field_to_treat = \"last_pymnt_amnt\"\n",
    "\n",
    "df_no_outliers = df_dropped[(z_scores[field_to_treat] < threshold)]\n",
    "df_no_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRfFah2lQgy3"
   },
   "outputs": [],
   "source": [
    "# Option 2 - Transforming field affected by outliers\n",
    "field_to_treat_log = f\"{field_to_treat}_log\"\n",
    "\n",
    "df_dropped[field_to_treat_log] = np.log1p(df_dropped[field_to_treat])\n",
    "print(f\"Maximum value of z score after taking log: {max(np.abs(stats.zscore(df_dropped[field_to_treat_log].dropna())))}\") # Added dropna() as some NAs returned (ideally to be fixed, for illustrative purposes only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JpirxjZQgy6"
   },
   "outputs": [],
   "source": [
    "# Option 3 - Capping outliers\n",
    "field_to_treat_capped = f\"{field_to_treat}_capped\"\n",
    "\n",
    "# e.g.: Capping outliers at 95th percentile\n",
    "cap_value = df_dropped[field_to_treat].quantile(0.95)\n",
    "print(f\"Value to be applied as cap: {cap_value}\")\n",
    "\n",
    "df_dropped[field_to_treat_capped] = np.where(df_dropped[field_to_treat] > cap_value, cap_value, df_dropped[field_to_treat])\n",
    "\n",
    "print(f\"Maximum value of z score after capping: {max(np.abs(stats.zscore(df_dropped[field_to_treat_capped])))}\")\n",
    "\n",
    "field_to_plot = field_to_treat_capped\n",
    "\n",
    "# Plotting variable distribution with outliers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=df_dropped[field_to_plot])\n",
    "plt.title(f\"Distribution and outliers for {field_to_plot}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9ivRrCOQgy6"
   },
   "source": [
    "Note: Capping will change the distribution, and as a result you may still see outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-1_yucyQgy7"
   },
   "source": [
    "#### Measuring Impact of Outlier Analysis\n",
    "\n",
    "Tracking the effectiveness of outlier analysis involves comparing model performance before and after handling outliers.\n",
    "\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "By detecting and handling outliers, we can improve the quality of the dataset and the performance of predictive models. In this example, we used the Z-score method for outlier detection and demonstrated various techniques for handling outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V12CRAapQgy7"
   },
   "source": [
    "### <span style=\"color:BLUE\"> **>>> DISCUSSION:**  </span>    \n",
    "- Can you think of benefits / limitations of above approaches?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZkDc1_lQgy7"
   },
   "source": [
    "6\\. Text data handling\n",
    "----------------------------------------\n",
    "\n",
    "Text data often requires cleaning, such as converting to lowercase, removing special characters, and trimming whitespace.\n",
    "\n",
    "Let us assume we needed to extract the loan ID from the url feature (not the case here, since this is already in the Id column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhZfibI5Qgy8"
   },
   "outputs": [],
   "source": [
    "df_dropped['url'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_qjYQfnQgy-"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_loan_id_from_url(mystring):\n",
    "    keyword = '='\n",
    "    before_keyword, keyword, after_keyword = mystring.partition(keyword)\n",
    "    return after_keyword\n",
    "\n",
    "df_dropped['loan_id_extracted'] = df_dropped['url'].apply(extract_loan_id_from_url)\n",
    "df_dropped[['id','url','loan_id_extracted']][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scQsZll4Qgy-"
   },
   "source": [
    "Let's check how employment title is populated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kRdarZtQgy_"
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique titles:\", df_dropped['emp_title'].value_counts().shape[0])\n",
    "df_dropped['emp_title'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0zaW5wJQgzB"
   },
   "source": [
    "This is likely too granular for us.\n",
    "Let's start by cleaning this list up and make it consistent (e.g.: \"Manager\" and \"manager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUwAOIcOQgzC"
   },
   "outputs": [],
   "source": [
    "# Cleaning text data\n",
    "df_dropped['emp_title_clean'] = df_dropped['emp_title'].str.lower().str.strip()\n",
    "df_dropped['emp_title_clean'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rmz_Kbz0QgzC"
   },
   "outputs": [],
   "source": [
    "df_dropped['emp_title_manager'] = df_dropped['emp_title_clean'].str.contains(\"manager\")\n",
    "df_dropped['emp_title_manager'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22NUp9aUQgzD"
   },
   "outputs": [],
   "source": [
    "selected_columns = df_dropped[[\"emp_title_manager\", \"emp_title_clean\"]]\n",
    "unique_combinations = selected_columns.drop_duplicates()\n",
    "unique_combinations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GApnDUtvQgzE"
   },
   "source": [
    "\n",
    "### Joining to external data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQlLCKGHQgzF"
   },
   "source": [
    "Even after this initial cleaning, the number of job families is still very granular for analysis (92!).\n",
    "The Lending Club business has created an internal mapping document provided (see `data/1-raw/emp_title_mapping.csv` file).\n",
    "\n",
    "We will now be joining our data to this reference file to be able to leverage the same categories adopted by the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8zIAiHfQgzG"
   },
   "outputs": [],
   "source": [
    "ref_job_families = pd.read_csv(rf\"{path_python_material}/data/1-raw/emp_title_mapping.csv\")\n",
    "ref_job_families.head()\n",
    "\n",
    "print(f\"Number of existing job families: {ref_job_families.Category.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAukr3PzQgzH"
   },
   "outputs": [],
   "source": [
    "df_mapped = pd.merge(left = df_dropped\n",
    "                     , right = ref_job_families\n",
    "                     , left_on = \"emp_title_clean\"\n",
    "                     , right_on= \"Job Title\"\n",
    "                     , how = \"left\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtjYn7shQgzH"
   },
   "source": [
    "Let's perform a basic quality check by counting number of records before and after join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aV_vItyLQgzH"
   },
   "outputs": [],
   "source": [
    "print(df_dropped.shape, df_mapped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyBmDlNiQgzI"
   },
   "source": [
    "The number of records has increased, which is a sign that something has gone wrong. **Can you think of a reason for it?**\n",
    "\n",
    "**Further reading**: (more on joins [here](https://www.atlassian.com/data/sql/sql-join-types-explained-visually) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luUoMym2eNNy"
   },
   "outputs": [],
   "source": [
    "# Let's check if there are any duplicates in the reference file:\n",
    "print(ref_job_families.shape)\n",
    "print(ref_job_families.drop_duplicates().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_job_families.groupby([\"Category\", \"Job Title\"]).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6bfP3pEeeEF"
   },
   "outputs": [],
   "source": [
    "# The problem can be seen by grouping the ref data by job title and category:\n",
    "row_count = ref_job_families.groupby([\"Category\", \"Job Title\"]).size().reset_index(name='count')\n",
    "row_count[row_count['count'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uekk8QqbQgzJ"
   },
   "outputs": [],
   "source": [
    "ref_job_families[ref_job_families[\"Job Title\"] == \"medical assistant\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-l1KPtmQgzK"
   },
   "source": [
    "There you go! There are duplicates in the reference files which we want to remove to ensure 1:1 mapping of job families to the reference file. Let's remove duplicates and join again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "408-i1FlQgzK"
   },
   "outputs": [],
   "source": [
    "ref_job_families_deduped = ref_job_families.drop_duplicates()\n",
    "print(ref_job_families.shape, ref_job_families_deduped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhzSrTq3QgzK"
   },
   "outputs": [],
   "source": [
    "df_mapped_deduped = pd.merge(left = df_dropped\n",
    "                     , right = ref_job_families_deduped\n",
    "                     , left_on = \"emp_title_clean\"\n",
    "                     , right_on= \"Job Title\"\n",
    "                     , how = \"left\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0zgcDupQgzL"
   },
   "outputs": [],
   "source": [
    "print(df_dropped.shape, df_mapped_deduped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapped_deduped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dh2FFMB6QgzM"
   },
   "source": [
    "The files are both of the same size now.\n",
    "\n",
    "This example showed the importance of checking for key data quality indicators like row count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQfM9FUPQgzO"
   },
   "source": [
    "### Data export\n",
    "Before moving to the last section on time series, let's export our data to the \"intermediate\" data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdbrju3OQgzP"
   },
   "outputs": [],
   "source": [
    "df_mapped_deduped.to_csv(rf\"{path_python_material}/data/2-intermediate/df_out_dsif2.csv\"\n",
    "                        , index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrlcVUzbQgzY"
   },
   "source": [
    "## 7. Time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpGvfh-cQgzY"
   },
   "source": [
    "Working with time series data in Python and Pandas involves several key considerations to ensure data is properly handled and cleaned for analysis and modeling. Here are the main aspects to focus on:\n",
    "\n",
    "### Date Parsing\n",
    "Ensure that the date/time information is correctly parsed and set as the DataFrame index for efficient time-based operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJaaRj5uQgza"
   },
   "outputs": [],
   "source": [
    "# Extracting features from date column\n",
    "df_dropped['issue_d'] = pd.to_datetime(df_dropped['issue_d'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3eRY7PEQgza"
   },
   "source": [
    "### Creation of time series\n",
    "A time series is just a pandas DataFrame or Series that has a time based index.\n",
    "Let's create a time series for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5B569aPQgzb"
   },
   "outputs": [],
   "source": [
    "ts_loans = pd.DataFrame(df_dropped.groupby(\"issue_d\").count()[\"id\"],\n",
    "                       index = None)\n",
    "ts_loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mn1XEV1aQgzd"
   },
   "outputs": [],
   "source": [
    "# Let's check type of our index is as expected\n",
    "ts_loans.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEEDED FOR FIX (define as monthly frequency)\n",
    "ts_loans.index = ts_loans.index.to_period('M')\n",
    "ts_loans.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvgR99CTQgzd"
   },
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wfb4Fxz6Qgze"
   },
   "source": [
    "Missing values in time series has a particular implication - as often we will be expecting or requiring continuous time series, where observations exist for each period (e.g. each day). This is not always the case, for instance when working with intermittent time series where observations may not occur in each period (e.g. for rare events like earthquake data or sales of a specific SKU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXmZbXdGQgze"
   },
   "outputs": [],
   "source": [
    "ts_loans.index.to_series().diff()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBicVyMZQgzf"
   },
   "source": [
    "When continuous data is expected, there are techniques to deal with missing values such as [interpolation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html), forward-fill or backward-fill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-Qb3zntQgzf"
   },
   "source": [
    "### Resampling\n",
    "Resampling involves changing the frequency of the time series data, which can be useful for aggregating or downsampling data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbnAZkJrQgzf"
   },
   "source": [
    "From [pydata](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling) :\n",
    "    \n",
    "    pandas has a simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications.\n",
    "    resample() is a time-based groupby, followed by a reduction method on each of its groups. See some cookbook examples for some advanced strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Y_awkkqQgzg"
   },
   "outputs": [],
   "source": [
    "# Resample to monthly frequency, using mean for aggregation\n",
    "ts_loans_yearly = ts_loans.resample('Y').sum() # Alternatively, use mean()\n",
    "ts_loans_yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEEDED FOR FIX if previously set to monthly frequency \n",
    "ts_loans_yearly.index = ts_loans_yearly.index.to_timestamp()\n",
    "ts_loans_yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkhZVBN-Qgzg"
   },
   "source": [
    "### Plotting time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOhXKDWaQgzg"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set the Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.lineplot(x=ts_loans_yearly.index, y=ts_loans_yearly['id'])\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Time Series Plot')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KtsdFsHQgzi"
   },
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cwyWBFnQgzi"
   },
   "source": [
    "This can be done both on our time series (e.g.: time-based features such as lags, rolling statistics, and date/time components), as well as on our original dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ucj3CdkgQgzj"
   },
   "outputs": [],
   "source": [
    "df_dropped['year'] = df_dropped['issue_d'].dt.year\n",
    "df_dropped['month'] = df_dropped['issue_d'].dt.month\n",
    "df_dropped['day'] = df_dropped['issue_d'].dt.day\n",
    "df_dropped['week'] = df_dropped['issue_d'].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqTQTSioQgzj"
   },
   "source": [
    "### Outliers and understanding time series components\n",
    "In the \"Time series modelling in Financial Services\" session in a few weeks time, we will be looking at how to handle outlier and also separate out the different components that make up time series data, such as trend and seasonality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTAypJt2Qgzl"
   },
   "source": [
    "## Further reading\n",
    "\n",
    "- Pandas library: https://pandas.pydata.org/docs/getting_started/index.html#getting-started\n",
    "- SQL vs. pandas: https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html\n",
    "- Pandas cheat sheet: https://www.datacamp.com/cheat-sheet/pandas-cheat-sheet-for-data-science-in-python\n",
    "- Other libraries for data quality: https://www.telm.ai/blog/8-essential-python-libraries-for-mastering-data-quality-checks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXMShFHNQgzl"
   },
   "source": [
    "# End of session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4dI1TQhQgzm"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=rf\"{path_python_material}/images/the-end.jpg\", width=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDKW6uPXQgzm"
   },
   "source": [
    "### <span style=\"color:BLUE\"> **>>> ADDITIONAL EXERCISES (optional):**  </span>\n",
    "Apply cleaning techniques to additional features and document your findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmRN4d1jQgzn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dsif",
   "language": "python",
   "name": "dsif"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
